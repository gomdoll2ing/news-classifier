# ============================================================================
# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================
print("="*80)
print("ğŸ”§ 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸")
print("="*80)
!pip install -q --upgrade datasets polars openai pandas tqdm
print("âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\n")

import os
import sys
import time
import numpy as np
import pandas as pd
from pathlib import Path
import shutil
from google.colab import drive
from tqdm.auto import tqdm
from openai import OpenAI, RateLimitError
from datasets import load_dataset
from concurrent.futures import ThreadPoolExecutor

# TQDMì´ Pandasì™€ ì˜ ì‘ë™í•˜ë„ë¡ ì„¤ì •
tqdm.pandas()

# ============================================================================
# 2. Google Drive ë§ˆìš´íŠ¸ ë° ê²½ë¡œ ì„¤ì • (â˜… 1000ê°œ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •)
# ============================================================================
print("\n" + "="*80)
print("ğŸ“ 2. Google Drive ë§ˆìš´íŠ¸ ë° ê²½ë¡œ ì„¤ì •")
print("="*80)

drive.mount('/content/drive', force_remount=False)

# --- ê²½ë¡œ ì„¤ì • ---
SAVE_BASE_PATH = Path("/content/drive/MyDrive/best_news_classifier")
SAVE_BASE_PATH.mkdir(parents=True, exist_ok=True)
print(f"âœ… ê¸°ë³¸ ì €ì¥ ê²½ë¡œ: {SAVE_BASE_PATH}")

ORIGINAL_DATA_PATH = "/content/drive/MyDrive/joongang_crawl/combined_articles.parquet"
if not Path(ORIGINAL_DATA_PATH).exists():
    ORIGINAL_DATA_PATH = "/content/drive/MyDrive/combined_articles.parquet"
if not Path(ORIGINAL_DATA_PATH).exists():
     raise FileNotFoundError(f"âŒ 'ì „ì²´' ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ORIGINAL_DATA_PATH}")

# (ì…ë ¥) ê¸°ì¡´ 500ê°œ ìƒ˜í”Œ íŒŒì¼
EXISTING_500_PATH = SAVE_BASE_PATH / "chatgpt_500_labels_1990_2019.parquet"
if not EXISTING_500_PATH.exists():
     raise FileNotFoundError(f"âŒ 500ê°œ ìƒ˜í”Œ íŒŒì¼({EXISTING_500_PATH.name})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# (â˜…ìˆ˜ì •â˜…) (ì¶œë ¥) 500ê°œë¥¼ ì¶”ê°€í•˜ì—¬ ìƒˆë¡œ ì €ì¥í•  1000ê°œ ìƒ˜í”Œ íŒŒì¼
FINAL_1000_PATH = SAVE_BASE_PATH / "chatgpt_1000_labels_1990_2019.parquet"
print(f"âœ… ê¸°ì¡´ 500ê°œ íŒŒì¼: {EXISTING_500_PATH.name}")
print(f"âœ… ìµœì¢… 1000ê°œ íŒŒì¼: {FINAL_1000_PATH.name}")


# ============================================================================
# 3. 'ì „ì²´' ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° 1990-2019ë…„ í•„í„°ë§
# ============================================================================
print("\n" + "="*80)
print("ğŸš€ 3. 'ì „ì²´' ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§")
print("="*80)

LOCAL_DATA_PATH = Path("/dev/shm/combined_articles.parquet")
if not LOCAL_DATA_PATH.exists():
    print(f"ğŸš€ ì „ì²´ ë°ì´í„° ë³µì‚¬ ì‹œì‘: Drive -> RAM Disk (/dev/shm/)...")
    shutil.copyfile(ORIGINAL_DATA_PATH, LOCAL_DATA_PATH)
    print("âœ… ë°ì´í„° ë³µì‚¬ ì™„ë£Œ")
else:
    print("âœ… (ìŠ¤í‚µ) RAM ë””ìŠ¤í¬ì— 'ì „ì²´' ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

print(f"... RAM ë””ìŠ¤í¬ì—ì„œ {LOCAL_DATA_PATH.name} ë¡œë“œ ì¤‘ ...")
all_data_dataset = load_dataset("parquet", data_files={"inference": str(LOCAL_DATA_PATH)}, split="inference")
print(f"âœ… 'ì „ì²´' ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_data_dataset):,} ê±´")

print(f"... {len(all_data_dataset):,} ê±´ì˜ ì „ì²´ ë°ì´í„°ì—ì„œ 1990~2019ë…„ ë°ì´í„° í•„í„°ë§ ì¤‘ ...")
def filter_past_data(example):
    try:
        yyyymmdd = float(example['yyyymmdd'])
        return 19900101 <= yyyymmdd <= 20191231
    except (TypeError, ValueError, AttributeError):
        return False

past_data_dataset = all_data_dataset.filter(filter_past_data, num_proc=4)
print(f"âœ… 1990~2019ë…„ ë°ì´í„° {len(past_data_dataset):,} ê±´ í•„í„°ë§ ì™„ë£Œ.")

# (â˜…ìˆ˜ì •) ì´ 1000ê°œ(500+500)ê°€ í•„ìš”í•˜ë¯€ë¡œ, 1000ê°œ ë¯¸ë§Œì¸ì§€ ì²´í¬
if len(past_data_dataset) < 1000:
     raise ValueError(f"í•„í„°ë§ëœ 1990-2019ë…„ ë°ì´í„°ê°€ 1000ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤ ({len(past_data_dataset)}ê±´).")


# ============================================================================
# 4. [â˜…ìˆ˜ì •â˜…] 500ê°œ ì‹ ê·œ ìƒ˜í”Œ ì¶”ì¶œ (ì¤‘ë³µ ì œì™¸)
# ============================================================================
print("\n" + "="*80)
print("ğŸ² 4. 500ê°œ ì‹ ê·œ ìƒ˜í”Œ ì¶”ì¶œ (ì¤‘ë³µ ì œì™¸)")
print("="*80)

df_existing_500 = pd.read_parquet(EXISTING_500_PATH)
print(f"âœ… ê¸°ì¡´ 500ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ.")

shuffled_dataset = past_data_dataset.shuffle(seed=42)

# (â˜…ìˆ˜ì •â˜…) 500ê°œ ì´í›„ì˜ 500ê°œë¥¼ ì„ íƒ (range(500, 1000))
print("... 1990-2019 ë°ì´í„°ì—ì„œ 500~1000ë²ˆì§¸ ìƒ˜í”Œ 500ê°œ ì¶”ì¶œ ì¤‘ ...")
new_samples_dataset = shuffled_dataset.select(range(500, 1000)) 
new_samples_df = new_samples_dataset.to_pandas()
print(f"âœ… ì‹ ê·œ 500ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ. (ì´ 1,000ê°œ ìƒ˜í”Œ ì¤€ë¹„)")


# ============================================================================
# 5. ChatGPT API ë³‘ë ¬ í˜¸ì¶œ (â˜…ìˆ˜ì •â˜… 500ê±´)
# ============================================================================
print("\n" + "="*80)
print("ğŸ¤– 5. ChatGPT API ë³‘ë ¬ í˜¸ì¶œ (ì‹ ê·œ 500ê±´)")
print("="*80)

# --- OpenAI API ì„¤ì • ---
from google.colab import userdata
try:
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
    llm_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    llm_model = "gpt-4o-mini"
except Exception as e:
    print("âŒ Colab 'ë¹„ë°€' íƒ­ì—ì„œ 'OPENAI_API_KEY'ë¥¼ ì„¤ì •í•˜ì„¸ìš”! (ì˜¤ë¥˜: {e})")
    sys.exit(1)

CATEGORIES = "[ê²½ì œ, ì •ì¹˜, ì‚¬íšŒ, êµ­ì œ, ë¬¸í™”/ìŠ¤í¬ì¸ , ê¸°íƒ€]"
SYSTEM_PROMPT = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê¸°ì‚¬ì˜ í—¤ë“œë¼ì¸ê³¼ ë³¸ë¬¸ì„ ì½ê³ , 6ê°œì˜ ì¹´í…Œê³ ë¦¬ ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
6ê°œ ì¹´í…Œê³ ë¦¬ëŠ” {CATEGORIES} ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ 6ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•´ì•¼ í•˜ë©°, ê·¸ ì™¸ì˜ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤."""

def get_chatgpt_label_with_retry(headline, content, max_retries=5):
    user_prompt = f"""[í—¤ë“œë¼ì¸]: {headline}
[ë³¸ë¬¸ ìš”ì•½]: {str(content)[:1000]}
[ë¶„ë¥˜]: """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": user_prompt}]
    result = {'label': 'ë¶„ë¥˜ì˜¤ë¥˜', 'prompt_tokens': 0, 'completion_tokens': 0}
    retries = 0
    wait_time = 5
    while retries < max_retries:
        try:
            response = llm_client.chat.completions.create(
                model=llm_model, messages=messages, temperature=0.0, max_tokens=20
            )
            answer = response.choices[0].message.content.strip()
            if answer in ['ê²½ì œ', 'ì •ì¹˜', 'ì‚¬íšŒ', 'êµ­ì œ', 'ë¬¸í™”/ìŠ¤í¬ì¸ ', 'ê¸°íƒ€']:
                result['label'] = answer
            else:
                result['label'] = "ë¶„ë¥˜ì‹¤íŒ¨"
            if response.usage:
                result['prompt_tokens'] = response.usage.prompt_tokens
                result['completion_tokens'] = response.usage.completion_tokens
            return result
        except RateLimitError as e:
            retries += 1
            if retries == max_retries:
                print(f"ğŸš¨ API ìµœëŒ€ ì¬ì‹œë„(5íšŒ) ì‹¤íŒ¨. [ì˜¤ë¥˜] {e}")
                return result
            print(f"ğŸ•’ 429 RateLimitError. {wait_time}ì´ˆ í›„ ì¬ì‹œë„... ({retries}/{max_retries})")
            time.sleep(wait_time)
            wait_time *= 2
        except Exception as e:
            print(f"ğŸš¨ API ê¸°íƒ€ ì˜¤ë¥˜. [ì˜¤ë¥˜] {e}")
            return result

# --- ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor) ---
MAX_WORKERS = 10 
results = []

print(f"... [ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘] ì‹ ê·œ 500ê°œ ìƒ˜í”Œ API í˜¸ì¶œ (Workers={MAX_WORKERS}) ...")
print("... 429 ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤ ...")

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    tasks_iter = ( (row['headline'], row['content']) for index, row in new_samples_df.iterrows() )
    results = list(tqdm(
        executor.map(lambda p: get_chatgpt_label_with_retry(p[0], p[1]), tasks_iter),
        total=len(new_samples_df), # (â˜…ìˆ˜ì •) 500ê°œ
        desc="API ë³‘ë ¬ í˜¸ì¶œ"
    ))

print("âœ… ì‹ ê·œ 500ê°œ API í˜¸ì¶œ ì™„ë£Œ!")


# ============================================================================
# 6. [â˜…ìˆ˜ì •â˜…] 500ê°œ(ê¸°ì¡´) + 500ê°œ(ì‹ ê·œ) ë³‘í•© ë° ì €ì¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ’¾ 6. 500ê°œ(ê¸°ì¡´) + 500ê°œ(ì‹ ê·œ) ë³‘í•© ë° ì €ì¥")
print("="*80)

# 1. 500ê°œ ì‹ ê·œ ê²°ê³¼(List[Dict])ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
df_new_results = pd.json_normalize(results)

# 2. ì›ë³¸ 500ê°œ ìƒ˜í”Œ(new_samples_df)ê³¼ ì‹ ê·œ ê²°ê³¼(df_new_results) ê²°í•©
df_new_labeled_500 = pd.concat([new_samples_df.reset_index(drop=True), df_new_results], axis=1)

# 3. 'label' -> 'chatgpt_label'ë¡œ ì´ë¦„ ë³€ê²½
df_new_labeled_500.rename(columns={'label': 'chatgpt_label'}, inplace=True)

# 4. ê¸°ì¡´ 500ê°œ(df_existing_500)ì™€ ì‹ ê·œ 500ê°œ(df_new_labeled_500) ìµœì¢… ë³‘í•©
df_total_1000 = pd.concat([df_existing_500, df_new_labeled_500], ignore_index=True)

print(f"âœ… ê¸°ì¡´ 500ê°œ + ì‹ ê·œ 500ê°œ = ì´ {len(df_total_1000)}ê°œ ë°ì´í„° ë³‘í•© ì™„ë£Œ.")

# 5. (â˜…ìˆ˜ì •â˜…) ìµœì¢… 1,000ê°œ íŒŒì¼ì„ Driveì— ì €ì¥
df_total_1000.to_parquet(FINAL_1000_PATH, index=False)
print(f"ğŸ’¾ ìµœì¢… 1,000ê°œ ìƒ˜í”Œì´ {FINAL_1000_PATH.name} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ============================================================================
# 7. ì‹ ê·œ 500ê±´ì— ëŒ€í•œ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚°
# ============================================================================
print("\n" + "="*80)
print("ğŸ’° 7. ì‹ ê·œ 500ê±´ì— ëŒ€í•œ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ê³„ì‚°")
print("="*80)

PRICE_INPUT_PER_1M_TOKENS = 0.15  # $0.15
PRICE_OUTPUT_PER_1M_TOKENS = 0.60 # $0.60

# (â˜…ìˆ˜ì •â˜…) 'df_new_labeled_500' (ì‹ ê·œ 500ê°œ) ê¸°ì¤€ìœ¼ë¡œë§Œ ê³„ì‚°
total_prompt_tokens = df_new_labeled_500['prompt_tokens'].sum()
total_completion_tokens = df_new_labeled_500['completion_tokens'].sum()
total_tokens = total_prompt_tokens + total_completion_tokens

cost_input = (total_prompt_tokens / 1_000_000) * PRICE_INPUT_PER_1M_TOKENS
cost_output = (total_completion_tokens / 1_000_000) * PRICE_OUTPUT_PER_1M_TOKENS
total_cost = cost_input + cost_output

print(f"--- [ì‹ ê·œ 500ê±´ ì‘ì—… ê²°ê³¼] ---")
print(f"ì´ ì‚¬ìš© í† í°: {total_tokens:,} ê°œ")
print(f" - ì…ë ¥ (Prompt) í† í°: {total_prompt_tokens:,} ê°œ")
print(f" - ì¶œë ¥ (Completion) í† í°: {total_completion_tokens:,} ê°œ")
print("\n--- ì˜ˆìƒ ë¹„ìš© (USD) ---")
print(f"ì…ë ¥ ë¹„ìš©: ${cost_input:.6f}")
print(f"ì¶œë ¥ ë¹„ìš©: ${cost_output:.6f}")
print(f"ì´ ë¹„ìš©:   ${total_cost:.6f} (ì•½ {total_cost * 1350:.0f} ì›)") # (í™˜ìœ¨ 1350ì› ê°€ì •)
print("="*80)

print("\nğŸ‰ ëª¨ë“  500ê°œ ì¶”ê°€ ë¼ë²¨ë§ ì‘ì—… ì™„ë£Œ! ğŸ‰")
