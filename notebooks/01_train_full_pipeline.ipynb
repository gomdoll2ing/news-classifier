# ============================================================================
# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# ============================================================================
print("="*80)
print("ğŸ”§ 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜")
print("="*80)
!pip install -q --upgrade transformers datasets accelerate scikit-learn polars

# í•œê¸€ ê¹¨ì§ í˜„ìƒ í•´ê²°
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf
print("âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\n")


# ============================================================================
# 2. ì „ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================
print("="*80)
print("ğŸ“š 2. ì „ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸")
print("="*80)
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import sys
import torch
import numpy as np
import pandas as pd
import polars as pl
from pathlib import Path
import pickle
import gc
import shutil
import time
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm

# Scikit-learn ëª¨ë“ˆ
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# Hugging Face í•µì‹¬ ëª¨ë“ˆ
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)

print(f"âœ… Torch version: {torch.__version__}")
print(f"âœ… CUDA available: {torch.cuda.is_available()}")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\n")

# (â˜… â˜… â˜… A100 ìµœì í™” â˜… â˜… â˜…)
# A100ì˜ ê³ ì„±ëŠ¥ CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° map/dataloaderì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì •ì˜
N_PROC = 16


# ============================================================================
# 3. Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„° ë¡œì»¬ ë³µì‚¬ (â˜… â˜… â˜… RAM ë””ìŠ¤í¬ ìµœì í™” â˜… â˜… â˜…)
# ============================================================================
print("="*80)
print("ğŸ“ 3. Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„° ë¡œì»¬ ë³µì‚¬ (RAM ë””ìŠ¤í¬)")
print("="*80)
drive.mount('/content/drive', force_remount=False)

SAVE_BASE_PATH = Path("/content/drive/MyDrive/best_news_classifier")
SAVE_BASE_PATH.mkdir(parents=True, exist_ok=True)
print(f"âœ… ì €ì¥ ê²½ë¡œ í™•ì¸: {SAVE_BASE_PATH}")

# ì›ë³¸ ë°ì´í„° ê²½ë¡œ íƒìƒ‰
DRIVE_BASE_PATH = Path("/content/drive/MyDrive/joongang_crawl")
DRIVE_FILE_PATH = DRIVE_BASE_PATH / "combined_articles.parquet"
if not DRIVE_FILE_PATH.exists():
    DRIVE_FILE_PATH = Path("/content/drive/MyDrive/combined_articles.parquet")
if not DRIVE_FILE_PATH.exists():
    raise FileNotFoundError("âŒ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ combined_articles.parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# (â˜… â˜… â˜… í•µì‹¬ ìˆ˜ì • â˜… â˜… â˜…)
# ëŠë¦° /content/ ë””ìŠ¤í¬ ëŒ€ì‹ , ì´ˆê³ ì† /dev/shm/ (RAM ë””ìŠ¤í¬)ì— ë³µì‚¬í•©ë‹ˆë‹¤.
# A100ì˜ I/O ë³‘ëª© í˜„ìƒ("ë­˜ í•´ë„ 1ì‹œê°„ 46ë¶„")ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì¡°ì¹˜ì…ë‹ˆë‹¤.
LOCAL_FILE_PATH = Path("/dev/shm/combined_articles.parquet") 

if not LOCAL_FILE_PATH.exists():
    print(f"ğŸš€ ë°ì´í„° ë³µì‚¬ ì‹œì‘: Drive -> RAM Disk (/dev/shm/)...")
    start_time = time.time()
    shutil.copyfile(DRIVE_FILE_PATH, LOCAL_FILE_PATH)
    print(f"âœ… ë°ì´í„° ë³µì‚¬ ì™„ë£Œ ({time.time() - start_time:.1f}ì´ˆ ì†Œìš”)")
else:
    print("âœ… (ìŠ¤í‚µ) RAM ë””ìŠ¤í¬ì— ì´ë¯¸ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë³µì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# RAM ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ ë””ìŠ¤í¬ I/Oê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
raw_dataset = load_dataset("parquet", data_files={"train": str(LOCAL_FILE_PATH)}, split="train")
print(f"âœ… Hugging Face Dataset ë¡œë“œ ì™„ë£Œ (RAM Mapped): {len(raw_dataset):,} ê±´")

# RAM ë””ìŠ¤í¬ ì‚¬ìš© í˜„í™© í™•ì¸
!df -h /dev/shm/


# ============================================================================
# 4. ìŠ¤ë§ˆíŠ¸í•œ ì‹œê³„ì—´(Time-Series) ë°ì´í„° ë¶„í• 
# ============================================================================
print("\n" + "="*80)
print("ğŸ“… 4. ìŠ¤ë§ˆíŠ¸ ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (RAM ìµœì í™”)")
print("="*80)

print("... ë‚ ì§œ ê¸°ë°˜ ì¸ë±ì‹± ê³„ì‚° ì¤‘ ...")
# ë‚ ì§œì™€ ë¼ë²¨ë§Œ ë¹ ë¥´ê²Œ ë¡œë“œ
meta_df = raw_dataset.select_columns(['yyyymmdd', 'sector1']).to_pandas()

valid_indices = meta_df[
    (meta_df['yyyymmdd'].notna()) &
    (meta_df['sector1'].notna()) &
    (meta_df['yyyymmdd'].astype(float) >= 20200101) &
    (meta_df['yyyymmdd'].astype(float) <= 20251231)
].index.to_numpy()

sorted_indices = valid_indices[np.argsort(meta_df.loc[valid_indices, 'yyyymmdd'].values)]

total_len = len(sorted_indices)
train_end = int(total_len * 0.8)
val_end = int(total_len * 0.9)

train_idx = sorted_indices[:train_end]
val_idx = sorted_indices[train_end:val_end]
test_idx = sorted_indices[val_end:]

print(f"âœ… ì „ì²´ ìœ íš¨ ë°ì´í„°: {total_len:,} ê±´")
print(f" - Train: {len(train_idx):,} ê±´ (80%)")
print(f" - Valid: {len(val_idx):,} ê±´ (10%)")
print(f" - Test : {len(test_idx):,} ê±´ (10%)")

del meta_df, valid_indices, sorted_indices
gc.collect()


# ============================================================================
# 5. [í•µì‹¬ ê°œì„ ] ë¼ë²¨ ì§‘ê³„(Aggregation) ë° ì¸ì½”ë”© (V2)
# ============================================================================
print("\n" + "="*80)
print("ğŸ·ï¸ 5. [í•µì‹¬ ê°œì„ ] ë¼ë²¨ ì§‘ê³„(Aggregation) ë° ì¸ì½”ë”© (V2)")
print("="*80)

def map_to_main_sector(sector_str):
    if not isinstance(sector_str, str):
        return 'ê¸°íƒ€' # None, NaN ë“± ì²˜ë¦¬
    if sector_str in ['ê²½ì œ', 'ë¶€ë™ì‚°', 'ëˆ ë²„ëŠ” ì¬ë¯¸', 'ë¨¸ë‹ˆë©']:
        return 'ê²½ì œ'
    if sector_str in ['ì •ì¹˜', 'ë” ë¶í•œ']:
        return 'ì •ì¹˜'
    if sector_str in ['ì‚¬íšŒ', 'í”¼í”Œ', 'ì„¸ìƒê³¼ í•¨ê»˜', 'ê°€ì¡±ê³¼ í•¨ê»˜', 'hello! Parents', 'í†¡í†¡ì—ë“€']:
        return 'ì‚¬íšŒ'
    if sector_str in ['êµ­ì œ', 'ë” ì°¨ì´ë‚˜']:
        return 'êµ­ì œ'
    if sector_str in ['ìŠ¤í¬ì¸ ', 'ë¬¸í™”', 'ì—¬í–‰ë ˆì €', 'COOKING', 'ì‰´ ë• ë­í•˜ì§€',
                      'ë”,ë§ˆìŒ', '2024 íŒŒë¦¬ì˜¬ë¦¼í”½', 'ë”,ì˜¤ë˜', 'ë” í—¬ìŠ¤', 'ë¼ì´í”„',
                      'ë§ˆìŒ ì±™ê¸°ê¸°', 'ë” í•˜ì´ì—”ë“œ', 'ë¹„í¬ë‹‰']:
        return 'ë¬¸í™”/ìŠ¤í¬ì¸ '
    if sector_str in ['ë¸Œëœë“œë‰´ìŠ¤', 'ì˜¤í”¼ë‹ˆì–¸', 'ì¤‘ì•™SUNDAY', 'Leader & Reader', 'Tran:D']:
        return 'ê¸°íƒ€'
    return 'ê¸°íƒ€'

# 5-1. Train Setì˜ ì›ë³¸ ë¼ë²¨ë¡œ ì§‘ê³„ ë° ì¸ì½”ë” í•™ìŠµ
print("... ì›ë³¸ ë¼ë²¨ì„ 6ê°œ ë©”ì¸ ì„¹í„°ë¡œ ì§‘ê³„ ì¤‘ ...")
train_labels_raw = raw_dataset.select(train_idx)['sector1']
agg_labels_raw = [map_to_main_sector(s) for s in train_labels_raw]

label_encoder = LabelEncoder()
label_encoder.fit(agg_labels_raw) # ì§‘ê³„ëœ ë¼ë²¨ë¡œ 'fit'
num_labels = len(label_encoder.classes_)
print(f"âœ… [ê°œì„ ] 33ê°œ ë¼ë²¨ -> {num_labels}ê°œ ë©”ì¸ ë¼ë²¨ë¡œ ì§‘ê³„ ì™„ë£Œ")
print(f"   (í´ë˜ìŠ¤: {label_encoder.classes_})")

# 5-2. ì•ˆì •ì ì¸ ì˜ë¬¸ ë¼ë²¨ ë§µ ìƒì„±
KOR_TO_ENG_MAP = {
    'ê²½ì œ': 'Economy',
    'êµ­ì œ': 'International',
    'ê¸°íƒ€': 'ETC',
    'ë¬¸í™”/ìŠ¤í¬ì¸ ': 'Culture/Sports',
    'ì‚¬íšŒ': 'Society',
    'ì •ì¹˜': 'Politics'
}
english_labels_ordered = [KOR_TO_ENG_MAP.get(k, k) for k in label_encoder.classes_]
print(f"   (ì˜ë¬¸ ë¼ë²¨: {english_labels_ordered})")


# 5-3. ì§‘ê³„ëœ ë¼ë²¨ ë¶„í¬ (ë¶ˆê· í˜• í™•ì¸)
print("\nğŸ“Š [ì¤‘ìš”] ì§‘ê³„ëœ ë¼ë²¨ ë¶„í¬ (Train Set):")
agg_dist_series = pd.Series(agg_labels_raw)
print(agg_dist_series.value_counts(normalize=True).to_string(float_format="{:.2%}".format))

del train_labels_raw, agg_labels_raw, agg_dist_series
gc.collect()

# 5-4. ìµœì¢… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì§‘ê³„ + ì¸ì½”ë”© í¬í•¨)
def preprocess_and_encode(examples):
    h = [t if t else "" for t in examples['headline']]
    c = [t if t else "" for t in examples['content']]
    texts = [f"{hh} {cc}"[:1000] for hh, cc in zip(h, c)] # 512 í† í°ì— ë§ì¶° ì ì ˆíˆ ìë¦„
    main_sectors = [map_to_main_sector(s) for s in examples['sector1']]
    labels = label_encoder.transform(main_sectors)
    return {'text': texts, 'label': labels}

# 5-5. ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ ì ìš©
train_ds = raw_dataset.select(train_idx)
val_ds = raw_dataset.select(val_idx)
test_ds = raw_dataset.select(test_idx)

print(f"\n... ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¼ë²¨ ì§‘ê³„ ì ìš© ì¤‘ (N_PROC={N_PROC})...")
train_ds_processed = train_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)
val_ds_processed = val_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)
test_ds_processed = test_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)
print("âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ (í…ìŠ¤íŠ¸ + 6ê°œ ì§‘ê³„ ë¼ë²¨)\n")


# ============================================================================
# 6. [ë² ì´ìŠ¤ë¼ì¸] TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€ (â˜… ìŠ¤í‚µ ê¸°ëŠ¥ ì ìš©)
# ============================================================================
print("="*80)
print("ğŸ”¬ 6. [ë² ì´ìŠ¤ë¼ì¸] TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€ (ìŠ¤í‚µ ê¸°ëŠ¥ ì ìš©)")
print("="*80)

LR_MODEL_PATH = SAVE_BASE_PATH / "baseline_lr.pkl"
LR_VEC_PATH = SAVE_BASE_PATH / "baseline_tfidf_vec.pkl"
LR_REPORT_PATH = SAVE_BASE_PATH / "baseline_report_lr.pkl"

X_test_text = test_ds_processed['text']
y_test_baseline = test_ds_processed['label']

if LR_MODEL_PATH.exists() and LR_VEC_PATH.exists() and LR_REPORT_PATH.exists():
    print("âœ… (ìŠ¤í‚µ) [6-2, 6-3] í›ˆë ¨ëœ TF-IDF ë° LR ëª¨ë¸ ë¡œë“œ...")
    with open(LR_VEC_PATH, 'rb') as f:
        tfidf_vectorizer = pickle.load(f)
    with open(LR_MODEL_PATH, 'rb') as f:
        lr_clf = pickle.load(f)
    with open(LR_REPORT_PATH, 'rb') as f:
        report_lr = pickle.load(f)

    print("... Test Setì— TF-IDF ì ìš© ì¤‘ ...")
    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)
    print("âœ… ë¡œë“œëœ ë²¡í„°ë¼ì´ì €ë¡œ Test Set ë³€í™˜ ì™„ë£Œ")

else:
    print("... [6-1] ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸/ë¼ë²¨ ì¶”ì¶œ ì¤‘ (ìµœì´ˆ 1íšŒ ì‹¤í–‰) ...")
    X_train_text = train_ds_processed['text']
    y_train = train_ds_processed['label']
    print(f"âœ… Train: {len(X_train_text):,} ê±´, Test: {len(X_test_text):,} ê±´ ë¡œë“œ ì™„ë£Œ")

    # 6-2. TF-IDF ë²¡í„°í™”
    print("\nğŸš€ [6-2] TF-IDF ë²¡í„°í™” ì¤‘ (Train Set ê¸°ì¤€)...")
    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_features=50000, sublinear_tf=True)
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)
    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)
    print(f"âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ. í”¼ì²˜ ê°œìˆ˜: {X_train_tfidf.shape[1]} ê°œ")

    # 6-3. ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ (ë¶ˆê· í˜• ì²˜ë¦¬)
    print("\nğŸš€ [6-3] ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...")
    lr_clf = LogisticRegression(
        solver='saga',
        C=1.0,
        max_iter=1000,
        n_jobs=-1,
        random_state=42,
        class_weight='balanced'
    )
    lr_clf.fit(X_train_tfidf, y_train)
    print("âœ… ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ ì™„ë£Œ (ë¶ˆê· í˜• ê°€ì¤‘ì¹˜ ì ìš©)")
    
    print(f"... ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì €ì¥ ì¤‘: {SAVE_BASE_PATH}")
    with open(LR_VEC_PATH, 'wb') as f:
        pickle.dump(tfidf_vectorizer, f)
    with open(LR_MODEL_PATH, 'wb') as f:
        pickle.dump(lr_clf, f)
    
    del X_train_text, y_train, X_train_tfidf
    gc.collect()

# 6-4. í‰ê°€ ë° ë¹„êµ
print("\nğŸ“Š [6-4] [TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€] Test Set í‰ê°€ ê²°ê³¼:")
y_pred_baseline = lr_clf.predict(X_test_tfidf)
all_labels_indices = np.arange(len(label_encoder.classes_))

if 'report_lr' not in locals():
    print("... [ìµœì´ˆ ì‹¤í–‰] Classification Report ìƒì„± ë° ì €ì¥ ...")
    report_lr = classification_report(
        y_test_baseline,
        y_pred_baseline,
        labels=all_labels_indices,
        target_names=label_encoder.classes_,
        digits=4,
        zero_division=0,
        output_dict=True
    )
    with open(LR_REPORT_PATH, 'wb') as f:
        pickle.dump(report_lr, f)

print(classification_report(
    y_test_baseline,
    y_pred_baseline,
    labels=all_labels_indices,
    target_names=label_encoder.classes_,
    digits=4,
    zero_division=0
))
print("\nâœ… ë² ì´ìŠ¤ë¼ì¸(LR) ëª¨ë¸ í‰ê°€ ê²°ê³¼ê°€ 'report_lr' ë³€ìˆ˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

del X_test_text, y_test_baseline, X_test_tfidf, y_pred_baseline
gc.collect()


# ============================================================================
# 7. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (â˜… ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ â˜…)
# ============================================================================
print("\n" + "="*80)
print("âš™ï¸ 7. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ëª¨ë¸ ë¡œë“œ (KLUE-RoBERTa)")
print("="*80)

MODEL_NAME = 'klue/roberta-base'
print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)
model.to(device)

# (â˜… â˜… â˜… í•µì‹¬ ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)
# klue/roberta-baseì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 514 (512 + 2íŠ¹ìˆ˜í† í°) ì…ë‹ˆë‹¤.
# 1024ë¡œ ì„¤ì •í•˜ë©´ 'CUDA device-side assert' ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.
def tokenize_text(examples):
    return tokenizer(
        examples['text'], 
        padding="max_length", 
        truncation=True, 
        max_length=512  # â˜… 1024 -> 512ë¡œ ìˆ˜ì •
    )

print(f"... í† í¬ë‚˜ì´ì§• ì§„í–‰ ì¤‘ (max_length=512, N_PROC={N_PROC})...")
train_ds_tokenized = train_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])
val_ds_tokenized = val_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])
test_ds_tokenized = test_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])

train_ds_tokenized.set_format("torch")
val_ds_tokenized.set_format("torch")
test_ds_tokenized.set_format("torch")
print("âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ\n")


# ============================================================================
# 8. [ë”¥ëŸ¬ë‹ ëª¨ë¸] íŒŒì¸íŠœë‹ (â˜… â˜… â˜… ì‹œìŠ¤í…œ ê¶Œì¥ ìµœì¢… íŠœë‹ â˜… â˜… â˜…)
# ============================================================================
print("="*80)
print("ğŸ”¥ 8. [ë”¥ëŸ¬ë‹ ëª¨ë¸] íŒŒì¸íŠœë‹ ì‹œì‘ (ì‹œìŠ¤í…œ ê¶Œì¥ íŠœë‹)")
print("="*80)

# (VRAM í´ë¦¬ì–´ ì½”ë“œ)
import gc
gc.collect()
torch.cuda.empty_cache()
print("âœ… ì´ì „ VRAM ìºì‹œë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤. 0GBì—ì„œ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    return {'accuracy': acc, 'f1': f1}

# [ìµœì í™”ëœ í•™ìŠµ íŒŒë¼ë¯¸í„°]
training_args = TrainingArguments(
    output_dir="/content/news-classifier-checkpoints",
    num_train_epochs=5,

    # (ìœ ì§€) VRAMì„ 75GB+ë¡œ ì±„ìš°ëŠ” ë°°ì¹˜
    per_device_train_batch_size=352,
    
    gradient_accumulation_steps=1, 

    learning_rate=3e-5,
    weight_decay=0.01,

    eval_strategy="epoch",
    save_strategy="epoch",

    logging_steps=100,

    load_best_model_at_end=True,
    metric_for_best_model="f1",

    fp16=True, 
    
    # (â˜… â˜… â˜… ìµœì¢… íŠœë‹ â˜… â˜… â˜…)
    # ì‹œìŠ¤í…œì´ 16ê°œëŠ” ê³¼ë„í•˜ê³  12ê°œê°€ ìµœì ì´ë¼ê³  ê²½ê³  (UserWarning)
    # ë”°ë¼ì„œ 16 -> 12ë¡œ ìˆ˜ì •
    dataloader_num_workers=12, 
    
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds_tokenized,
    eval_dataset=val_ds_tokenized,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

print(f"ğŸš€ í•™ìŠµ ì‹œì‘ (Batch: {training_args.per_device_train_batch_size}, Workers: {training_args.dataloader_num_workers}, fp16: {training_args.fp16})...")
trainer.train()
print("âœ… í•™ìŠµ ì™„ë£Œ!\n")


# ============================================================================
# 9. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ìµœì¢… í‰ê°€
# ============================================================================
print("="*80)
print("ğŸ† 9. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ìµœì¢… ëª¨ë¸ í‰ê°€")
print("="*80)

print("... ë”¥ëŸ¬ë‹ ëª¨ë¸ Test Set í‰ê°€ ì¤‘ ...")
test_results = trainer.evaluate(test_ds_tokenized)
print(f"\nğŸ¯ Test Set ì •í™•ë„: {test_results['eval_accuracy']:.4f}")
print(f"ğŸ¯ Test Set F1ì ìˆ˜: {test_results['eval_f1']:.4f}\n")

print("... ë”¥ëŸ¬ë‹ ëª¨ë¸ Test Set ì˜ˆì¸¡(Prediction) ìƒì„± ì¤‘ ...")
dl_predictions = trainer.predict(test_ds_tokenized)
y_pred_dl = np.argmax(dl_predictions.predictions, axis=1)
y_true_dl = dl_predictions.label_ids

# Classification Report ìƒì„±
report_dl = classification_report(
    y_true_dl,
    y_pred_dl,
    labels=all_labels_indices,
    target_names=label_encoder.classes_,
    digits=4,
    zero_division=0,
    output_dict=True
)
# í™”ë©´ì—ë„ ë¦¬í¬íŠ¸ ì¶œë ¥
print("\nğŸ“Š [KLUE-RoBERTa] Test Set í‰ê°€ ê²°ê³¼:")
print(classification_report(
    y_true_dl,
    y_pred_dl,
    labels=all_labels_indices,
    target_names=label_encoder.classes_,
    digits=4,
    zero_division=0
))
print("\nâœ… ë”¥ëŸ¬ë‹(RoBERTa) ëª¨ë¸ í‰ê°€ ê²°ê³¼ê°€ 'report_dl' ë³€ìˆ˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ============================================================================
# 10. [ğŸŒŸ ì‹ ê·œ] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™” (â˜… â˜… â˜… ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)
# ============================================================================
print("\n" + "="*80)
print("ğŸ“Š 10. [ì¢…í•©] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™”")
print("="*80)

# (â˜… ë©”ëª¨ë¦¬ ìµœì í™”) 6ë²ˆì—ì„œ ì‚­ì œëœ ë³€ìˆ˜ë“¤ ì¬ì‚¬ìš©ì„ ìœ„í•´ ì¬ìƒì„±
print("... ë² ì´ìŠ¤ë¼ì¸(LR) ëª¨ë¸ ì˜ˆì¸¡ê°’ ì¬ìƒì„± (ì‹œê°í™”ìš©) ...")
X_test_text_viz = test_ds_processed['text']
y_true_baseline_viz = test_ds_processed['label']
X_test_tfidf_viz = tfidf_vectorizer.transform(X_test_text_viz)
y_pred_baseline_viz = lr_clf.predict(X_test_tfidf_viz)
del X_test_text_viz, X_test_tfidf_viz # ì‚¬ìš© í›„ ì¦‰ì‹œ ì‚­ì œ
gc.collect()
print("âœ… ë² ì´ìŠ¤ë¼ì¸ ì˜ˆì¸¡ê°’ ì¤€ë¹„ ì™„ë£Œ")


# 10-1. ì¢…í•© ì„±ëŠ¥ ë¹„êµí‘œ (DataFrame)
print("\n### 1. ì¢…í•© ì„±ëŠ¥ ë¹„êµí‘œ (Weighted Avg.)\n")
metrics_lr = report_lr['weighted avg']
metrics_dl = report_dl['weighted avg']
comparison_df = pd.DataFrame({
    'Baseline (TF-IDF + LR)': metrics_lr,
    'KLUE-RoBERTa': metrics_dl
}).T[['precision', 'recall', 'f1-score', 'support']]
print(comparison_df.to_string(float_format="{:.4f}".format))


# 10-2. ì¢…í•© ì„±ëŠ¥ ì‹œê°í™” (Bar Chart)
print("\n\n### 2. ì¢…í•© ì„±ëŠ¥ ì‹œê°í™” (F1 / Precision / Recall)\n")
df_long = comparison_df.drop(columns='support').reset_index().melt(
    'index', var_name='Metric', value_name='Score'
)
df_long.rename(columns={'index': 'Model'}, inplace=True)
plt.figure(figsize=(10, 6))
sns.barplot(data=df_long, x='Metric', y='Score', hue='Model', palette='muted')
plt.title('Baseline vs. KLUE-RoBERTa (Weighted Avg. Metrics)', fontsize=16, fontweight='bold')
plt.ylabel('Score')
plt.xlabel('Metric')
plt.legend(title='Model', loc='lower right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()


# 10-3. Side-by-Side í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ë¹„êµ
print("\n\n### 3. ëª¨ë¸ë³„ í˜¼ë™ í–‰ë ¬ ë¹„êµ (Normalized)\n")

# (â˜… â˜… â˜… ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)
# 'ticklabels='ë¥¼ 'xticklabels='ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.
def plot_confusion_matrix_custom(ax, y_true, y_pred, classes, title):
    cm = confusion_matrix(y_true, y_pred, normalize='true')
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', square=True,
                xticklabels=classes, # â˜… ìˆ˜ì •ë¨
                yticklabels=classes, 
                cbar=False, ax=ax)
    ax.set_ylabel('Actual Label', fontsize=12, fontweight='bold')
    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=14, pad=20)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# 1ë²ˆ í”Œë¡¯: ë² ì´ìŠ¤ë¼ì¸ (LR)
plot_confusion_matrix_custom(
    axes[0],
    y_true_baseline_viz,
    y_pred_baseline_viz,
    english_labels_ordered,
    'Baseline (TF-IDF + LR)'
)

# 2ë²ˆ í”Œë¡¯: ë”¥ëŸ¬ë‹ (RoBERTa)
plot_confusion_matrix_custom(
    axes[1],
    y_true_dl,
    y_pred_dl,
    english_labels_ordered,
    'KLUE-RoBERTa'
)

fig.suptitle('Model Comparison: Confusion Matrix (Normalized)', fontsize=18, fontweight='bold', y=1.05)
plt.tight_layout()
plt.show()

del y_true_baseline_viz, y_pred_baseline_viz, y_true_dl, y_pred_dl
gc.collect()


# ============================================================================
# 11. ìµœì¢… ëª¨ë¸ ì €ì¥
# ============================================================================
print("\n" + "="*80)
print("ğŸ† 11. ìµœì¢… ëª¨ë¸ ì €ì¥")
print("="*80)

trainer.save_model(str(SAVE_BASE_PATH))
tokenizer.save_pretrained(str(SAVE_BASE_PATH))
with open(SAVE_BASE_PATH / "label_encoder.pkl", 'wb') as f:
    pickle.dump(label_encoder, f)

print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_BASE_PATH}")

# ë¶ˆí•„ìš”í•œ ì²´í¬í¬ì¸íŠ¸ í´ë” ì‚­ì œ
shutil.rmtree("/content/news-classifier-checkpoints", ignore_errors=True)
print("âœ… ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ")
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
