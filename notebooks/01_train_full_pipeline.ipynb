{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qJTz01hzeA3"
      },
      "source": [
        "## 1. Bert Classification (ì‹ ë¬¸ ì£¼ì œ ë¶„ë¥˜ê¸°)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5DMzkkq-Cqr",
        "outputId": "c3bc3274-0bb9-46f6-c57a-806f4e28d68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/news-classifier/notebooks/01_train_full_pipeline.ipynb\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ”§ 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\")\n",
        "print(\"=\"*80)\n",
        "!pip install -q --upgrade transformers datasets accelerate scikit-learn polars\n",
        "\n",
        "# í•œê¸€ ê¹¨ì§ í˜„ìƒ í•´ê²°\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ì „ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ“š 2. ì „ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\")\n",
        "print(\"=\"*80)\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gc\n",
        "import shutil\n",
        "import time\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Scikit-learn ëª¨ë“ˆ\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Hugging Face í•µì‹¬ ëª¨ë“ˆ\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "print(f\"âœ… Torch version: {torch.__version__}\")\n",
        "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\\n\")\n",
        "\n",
        "# (â˜… â˜… â˜… A100 ìµœì í™” â˜… â˜… â˜…)\n",
        "# A100ì˜ ê³ ì„±ëŠ¥ CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° map/dataloaderì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì •ì˜\n",
        "N_PROC = 16\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„° ë¡œì»¬ ë³µì‚¬ (â˜… â˜… â˜… RAM ë””ìŠ¤í¬ ìµœì í™” â˜… â˜… â˜…)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ“ 3. Google Drive ë§ˆìš´íŠ¸ ë° ë°ì´í„° ë¡œì»¬ ë³µì‚¬ (RAM ë””ìŠ¤í¬)\")\n",
        "print(\"=\"*80)\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "SAVE_BASE_PATH = Path(\"/content/drive/MyDrive/best_news_classifier\")\n",
        "SAVE_BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"âœ… ì €ì¥ ê²½ë¡œ í™•ì¸: {SAVE_BASE_PATH}\")\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„° ê²½ë¡œ íƒìƒ‰\n",
        "DRIVE_BASE_PATH = Path(\"/content/drive/MyDrive/joongang_crawl\")\n",
        "DRIVE_FILE_PATH = DRIVE_BASE_PATH / \"combined_articles.parquet\"\n",
        "if not DRIVE_FILE_PATH.exists():\n",
        "    DRIVE_FILE_PATH = Path(\"/content/drive/MyDrive/combined_articles.parquet\")\n",
        "if not DRIVE_FILE_PATH.exists():\n",
        "    raise FileNotFoundError(\"âŒ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ combined_articles.parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# (â˜… â˜… â˜… í•µì‹¬ ìˆ˜ì • â˜… â˜… â˜…)\n",
        "# ëŠë¦° /content/ ë””ìŠ¤í¬ ëŒ€ì‹ , ì´ˆê³ ì† /dev/shm/ (RAM ë””ìŠ¤í¬)ì— ë³µì‚¬í•©ë‹ˆë‹¤.\n",
        "# A100ì˜ I/O ë³‘ëª© í˜„ìƒ(\"ë­˜ í•´ë„ 1ì‹œê°„ 46ë¶„\")ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì¡°ì¹˜ì…ë‹ˆë‹¤.\n",
        "LOCAL_FILE_PATH = Path(\"/dev/shm/combined_articles.parquet\")\n",
        "\n",
        "if not LOCAL_FILE_PATH.exists():\n",
        "    print(f\"ğŸš€ ë°ì´í„° ë³µì‚¬ ì‹œì‘: Drive -> RAM Disk (/dev/shm/)...\")\n",
        "    start_time = time.time()\n",
        "    shutil.copyfile(DRIVE_FILE_PATH, LOCAL_FILE_PATH)\n",
        "    print(f\"âœ… ë°ì´í„° ë³µì‚¬ ì™„ë£Œ ({time.time() - start_time:.1f}ì´ˆ ì†Œìš”)\")\n",
        "else:\n",
        "    print(\"âœ… (ìŠ¤í‚µ) RAM ë””ìŠ¤í¬ì— ì´ë¯¸ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë³µì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "\n",
        "# RAM ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ ë””ìŠ¤í¬ I/Oê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "raw_dataset = load_dataset(\"parquet\", data_files={\"train\": str(LOCAL_FILE_PATH)}, split=\"train\")\n",
        "print(f\"âœ… Hugging Face Dataset ë¡œë“œ ì™„ë£Œ (RAM Mapped): {len(raw_dataset):,} ê±´\")\n",
        "\n",
        "# RAM ë””ìŠ¤í¬ ì‚¬ìš© í˜„í™© í™•ì¸\n",
        "!df -h /dev/shm/\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ìŠ¤ë§ˆíŠ¸í•œ ì‹œê³„ì—´(Time-Series) ë°ì´í„° ë¶„í• \n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“… 4. ìŠ¤ë§ˆíŠ¸ ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (RAM ìµœì í™”)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"... ë‚ ì§œ ê¸°ë°˜ ì¸ë±ì‹± ê³„ì‚° ì¤‘ ...\")\n",
        "# ë‚ ì§œì™€ ë¼ë²¨ë§Œ ë¹ ë¥´ê²Œ ë¡œë“œ\n",
        "meta_df = raw_dataset.select_columns(['yyyymmdd', 'sector1']).to_pandas()\n",
        "\n",
        "valid_indices = meta_df[\n",
        "    (meta_df['yyyymmdd'].notna()) &\n",
        "    (meta_df['sector1'].notna()) &\n",
        "    (meta_df['yyyymmdd'].astype(float) >= 20200101) &\n",
        "    (meta_df['yyyymmdd'].astype(float) <= 20251231)\n",
        "].index.to_numpy()\n",
        "\n",
        "sorted_indices = valid_indices[np.argsort(meta_df.loc[valid_indices, 'yyyymmdd'].values)]\n",
        "\n",
        "total_len = len(sorted_indices)\n",
        "train_end = int(total_len * 0.8)\n",
        "val_end = int(total_len * 0.9)\n",
        "\n",
        "train_idx = sorted_indices[:train_end]\n",
        "val_idx = sorted_indices[train_end:val_end]\n",
        "test_idx = sorted_indices[val_end:]\n",
        "\n",
        "print(f\"âœ… ì „ì²´ ìœ íš¨ ë°ì´í„°: {total_len:,} ê±´\")\n",
        "print(f\" - Train: {len(train_idx):,} ê±´ (80%)\")\n",
        "print(f\" - Valid: {len(val_idx):,} ê±´ (10%)\")\n",
        "print(f\" - Test : {len(test_idx):,} ê±´ (10%)\")\n",
        "\n",
        "del meta_df, valid_indices, sorted_indices\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. [í•µì‹¬ ê°œì„ ] ë¼ë²¨ ì§‘ê³„(Aggregation) ë° ì¸ì½”ë”© (V2)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ·ï¸ 5. [í•µì‹¬ ê°œì„ ] ë¼ë²¨ ì§‘ê³„(Aggregation) ë° ì¸ì½”ë”© (V2)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def map_to_main_sector(sector_str):\n",
        "    if not isinstance(sector_str, str):\n",
        "        return 'ê¸°íƒ€' # None, NaN ë“± ì²˜ë¦¬\n",
        "    if sector_str in ['ê²½ì œ', 'ë¶€ë™ì‚°', 'ëˆ ë²„ëŠ” ì¬ë¯¸', 'ë¨¸ë‹ˆë©']:\n",
        "        return 'ê²½ì œ'\n",
        "    if sector_str in ['ì •ì¹˜', 'ë” ë¶í•œ']:\n",
        "        return 'ì •ì¹˜'\n",
        "    if sector_str in ['ì‚¬íšŒ', 'í”¼í”Œ', 'ì„¸ìƒê³¼ í•¨ê»˜', 'ê°€ì¡±ê³¼ í•¨ê»˜', 'hello! Parents', 'í†¡í†¡ì—ë“€']:\n",
        "        return 'ì‚¬íšŒ'\n",
        "    if sector_str in ['êµ­ì œ', 'ë” ì°¨ì´ë‚˜']:\n",
        "        return 'êµ­ì œ'\n",
        "    if sector_str in ['ìŠ¤í¬ì¸ ', 'ë¬¸í™”', 'ì—¬í–‰ë ˆì €', 'COOKING', 'ì‰´ ë• ë­í•˜ì§€',\n",
        "                      'ë”,ë§ˆìŒ', '2024 íŒŒë¦¬ì˜¬ë¦¼í”½', 'ë”,ì˜¤ë˜', 'ë” í—¬ìŠ¤', 'ë¼ì´í”„',\n",
        "                      'ë§ˆìŒ ì±™ê¸°ê¸°', 'ë” í•˜ì´ì—”ë“œ', 'ë¹„í¬ë‹‰']:\n",
        "        return 'ë¬¸í™”/ìŠ¤í¬ì¸ '\n",
        "    if sector_str in ['ë¸Œëœë“œë‰´ìŠ¤', 'ì˜¤í”¼ë‹ˆì–¸', 'ì¤‘ì•™SUNDAY', 'Leader & Reader', 'Tran:D']:\n",
        "        return 'ê¸°íƒ€'\n",
        "    return 'ê¸°íƒ€'\n",
        "\n",
        "# 5-1. Train Setì˜ ì›ë³¸ ë¼ë²¨ë¡œ ì§‘ê³„ ë° ì¸ì½”ë” í•™ìŠµ\n",
        "print(\"... ì›ë³¸ ë¼ë²¨ì„ 6ê°œ ë©”ì¸ ì„¹í„°ë¡œ ì§‘ê³„ ì¤‘ ...\")\n",
        "train_labels_raw = raw_dataset.select(train_idx)['sector1']\n",
        "agg_labels_raw = [map_to_main_sector(s) for s in train_labels_raw]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(agg_labels_raw) # ì§‘ê³„ëœ ë¼ë²¨ë¡œ 'fit'\n",
        "num_labels = len(label_encoder.classes_)\n",
        "print(f\"âœ… [ê°œì„ ] 33ê°œ ë¼ë²¨ -> {num_labels}ê°œ ë©”ì¸ ë¼ë²¨ë¡œ ì§‘ê³„ ì™„ë£Œ\")\n",
        "print(f\"   (í´ë˜ìŠ¤: {label_encoder.classes_})\")\n",
        "\n",
        "# 5-2. ì•ˆì •ì ì¸ ì˜ë¬¸ ë¼ë²¨ ë§µ ìƒì„±\n",
        "KOR_TO_ENG_MAP = {\n",
        "    'ê²½ì œ': 'Economy',\n",
        "    'êµ­ì œ': 'International',\n",
        "    'ê¸°íƒ€': 'ETC',\n",
        "    'ë¬¸í™”/ìŠ¤í¬ì¸ ': 'Culture/Sports',\n",
        "    'ì‚¬íšŒ': 'Society',\n",
        "    'ì •ì¹˜': 'Politics'\n",
        "}\n",
        "english_labels_ordered = [KOR_TO_ENG_MAP.get(k, k) for k in label_encoder.classes_]\n",
        "print(f\"   (ì˜ë¬¸ ë¼ë²¨: {english_labels_ordered})\")\n",
        "\n",
        "\n",
        "# 5-3. ì§‘ê³„ëœ ë¼ë²¨ ë¶„í¬ (ë¶ˆê· í˜• í™•ì¸)\n",
        "print(\"\\nğŸ“Š [ì¤‘ìš”] ì§‘ê³„ëœ ë¼ë²¨ ë¶„í¬ (Train Set):\")\n",
        "agg_dist_series = pd.Series(agg_labels_raw)\n",
        "print(agg_dist_series.value_counts(normalize=True).to_string(float_format=\"{:.2%}\".format))\n",
        "\n",
        "del train_labels_raw, agg_labels_raw, agg_dist_series\n",
        "gc.collect()\n",
        "\n",
        "# 5-4. ìµœì¢… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì§‘ê³„ + ì¸ì½”ë”© í¬í•¨)\n",
        "def preprocess_and_encode(examples):\n",
        "    h = [t if t else \"\" for t in examples['headline']]\n",
        "    c = [t if t else \"\" for t in examples['content']]\n",
        "    texts = [f\"{hh} {cc}\"[:1000] for hh, cc in zip(h, c)] # 512 í† í°ì— ë§ì¶° ì ì ˆíˆ ìë¦„\n",
        "    main_sectors = [map_to_main_sector(s) for s in examples['sector1']]\n",
        "    labels = label_encoder.transform(main_sectors)\n",
        "    return {'text': texts, 'label': labels}\n",
        "\n",
        "# 5-5. ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ ì ìš©\n",
        "train_ds = raw_dataset.select(train_idx)\n",
        "val_ds = raw_dataset.select(val_idx)\n",
        "test_ds = raw_dataset.select(test_idx)\n",
        "\n",
        "print(f\"\\n... ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ë¼ë²¨ ì§‘ê³„ ì ìš© ì¤‘ (N_PROC={N_PROC})...\")\n",
        "train_ds_processed = train_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)\n",
        "val_ds_processed = val_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)\n",
        "test_ds_processed = test_ds.map(preprocess_and_encode, batched=True, num_proc=N_PROC, remove_columns=raw_dataset.column_names)\n",
        "print(\"âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ (í…ìŠ¤íŠ¸ + 6ê°œ ì§‘ê³„ ë¼ë²¨)\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. [ë² ì´ìŠ¤ë¼ì¸] TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€ (â˜… ìŠ¤í‚µ ê¸°ëŠ¥ ì ìš©)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ”¬ 6. [ë² ì´ìŠ¤ë¼ì¸] TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€ (ìŠ¤í‚µ ê¸°ëŠ¥ ì ìš©)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "LR_MODEL_PATH = SAVE_BASE_PATH / \"baseline_lr.pkl\"\n",
        "LR_VEC_PATH = SAVE_BASE_PATH / \"baseline_tfidf_vec.pkl\"\n",
        "LR_REPORT_PATH = SAVE_BASE_PATH / \"baseline_report_lr.pkl\"\n",
        "\n",
        "X_test_text = test_ds_processed['text']\n",
        "y_test_baseline = test_ds_processed['label']\n",
        "\n",
        "if LR_MODEL_PATH.exists() and LR_VEC_PATH.exists() and LR_REPORT_PATH.exists():\n",
        "    print(\"âœ… (ìŠ¤í‚µ) [6-2, 6-3] í›ˆë ¨ëœ TF-IDF ë° LR ëª¨ë¸ ë¡œë“œ...\")\n",
        "    with open(LR_VEC_PATH, 'rb') as f:\n",
        "        tfidf_vectorizer = pickle.load(f)\n",
        "    with open(LR_MODEL_PATH, 'rb') as f:\n",
        "        lr_clf = pickle.load(f)\n",
        "    with open(LR_REPORT_PATH, 'rb') as f:\n",
        "        report_lr = pickle.load(f)\n",
        "\n",
        "    print(\"... Test Setì— TF-IDF ì ìš© ì¤‘ ...\")\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
        "    print(\"âœ… ë¡œë“œëœ ë²¡í„°ë¼ì´ì €ë¡œ Test Set ë³€í™˜ ì™„ë£Œ\")\n",
        "\n",
        "else:\n",
        "    print(\"... [6-1] ë°ì´í„°ì…‹ì—ì„œ í…ìŠ¤íŠ¸/ë¼ë²¨ ì¶”ì¶œ ì¤‘ (ìµœì´ˆ 1íšŒ ì‹¤í–‰) ...\")\n",
        "    X_train_text = train_ds_processed['text']\n",
        "    y_train = train_ds_processed['label']\n",
        "    print(f\"âœ… Train: {len(X_train_text):,} ê±´, Test: {len(X_test_text):,} ê±´ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "    # 6-2. TF-IDF ë²¡í„°í™”\n",
        "    print(\"\\nğŸš€ [6-2] TF-IDF ë²¡í„°í™” ì¤‘ (Train Set ê¸°ì¤€)...\")\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_features=50000, sublinear_tf=True)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
        "    print(f\"âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ. í”¼ì²˜ ê°œìˆ˜: {X_train_tfidf.shape[1]} ê°œ\")\n",
        "\n",
        "    # 6-3. ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ (ë¶ˆê· í˜• ì²˜ë¦¬)\n",
        "    print(\"\\nğŸš€ [6-3] ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\")\n",
        "    lr_clf = LogisticRegression(\n",
        "        solver='saga',\n",
        "        C=1.0,\n",
        "        max_iter=1000,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    lr_clf.fit(X_train_tfidf, y_train)\n",
        "    print(\"âœ… ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ ì™„ë£Œ (ë¶ˆê· í˜• ê°€ì¤‘ì¹˜ ì ìš©)\")\n",
        "\n",
        "    print(f\"... ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì €ì¥ ì¤‘: {SAVE_BASE_PATH}\")\n",
        "    with open(LR_VEC_PATH, 'wb') as f:\n",
        "        pickle.dump(tfidf_vectorizer, f)\n",
        "    with open(LR_MODEL_PATH, 'wb') as f:\n",
        "        pickle.dump(lr_clf, f)\n",
        "\n",
        "    del X_train_text, y_train, X_train_tfidf\n",
        "    gc.collect()\n",
        "\n",
        "# 6-4. í‰ê°€ ë° ë¹„êµ\n",
        "print(\"\\nğŸ“Š [6-4] [TF-IDF + ë¡œì§€ìŠ¤í‹± íšŒê·€] Test Set í‰ê°€ ê²°ê³¼:\")\n",
        "y_pred_baseline = lr_clf.predict(X_test_tfidf)\n",
        "all_labels_indices = np.arange(len(label_encoder.classes_))\n",
        "\n",
        "if 'report_lr' not in locals():\n",
        "    print(\"... [ìµœì´ˆ ì‹¤í–‰] Classification Report ìƒì„± ë° ì €ì¥ ...\")\n",
        "    report_lr = classification_report(\n",
        "        y_test_baseline,\n",
        "        y_pred_baseline,\n",
        "        labels=all_labels_indices,\n",
        "        target_names=label_encoder.classes_,\n",
        "        digits=4,\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "    with open(LR_REPORT_PATH, 'wb') as f:\n",
        "        pickle.dump(report_lr, f)\n",
        "\n",
        "print(classification_report(\n",
        "    y_test_baseline,\n",
        "    y_pred_baseline,\n",
        "    labels=all_labels_indices,\n",
        "    target_names=label_encoder.classes_,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "print(\"\\nâœ… ë² ì´ìŠ¤ë¼ì¸(LR) ëª¨ë¸ í‰ê°€ ê²°ê³¼ê°€ 'report_lr' ë³€ìˆ˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "del X_test_text, y_test_baseline, X_test_tfidf, y_pred_baseline\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 7. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (â˜… ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ â˜…)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âš™ï¸ 7. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ëª¨ë¸ ë¡œë“œ (KLUE-RoBERTa)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME = 'klue/roberta-base'\n",
        "print(f\"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {MODEL_NAME}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# (â˜… â˜… â˜… í•µì‹¬ ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)\n",
        "# klue/roberta-baseì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 514 (512 + 2íŠ¹ìˆ˜í† í°) ì…ë‹ˆë‹¤.\n",
        "# 1024ë¡œ ì„¤ì •í•˜ë©´ 'CUDA device-side assert' ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
        "def tokenize_text(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512  # â˜… 1024 -> 512ë¡œ ìˆ˜ì •\n",
        "    )\n",
        "\n",
        "print(f\"... í† í¬ë‚˜ì´ì§• ì§„í–‰ ì¤‘ (max_length=512, N_PROC={N_PROC})...\")\n",
        "train_ds_tokenized = train_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])\n",
        "val_ds_tokenized = val_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])\n",
        "test_ds_tokenized = test_ds_processed.map(tokenize_text, batched=True, num_proc=N_PROC, remove_columns=['text'])\n",
        "\n",
        "train_ds_tokenized.set_format(\"torch\")\n",
        "val_ds_tokenized.set_format(\"torch\")\n",
        "test_ds_tokenized.set_format(\"torch\")\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 8. [ë”¥ëŸ¬ë‹ ëª¨ë¸] íŒŒì¸íŠœë‹ (â˜… â˜… â˜… ì‹œìŠ¤í…œ ê¶Œì¥ ìµœì¢… íŠœë‹ â˜… â˜… â˜…)\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ”¥ 8. [ë”¥ëŸ¬ë‹ ëª¨ë¸] íŒŒì¸íŠœë‹ ì‹œì‘ (ì‹œìŠ¤í…œ ê¶Œì¥ íŠœë‹)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# (VRAM í´ë¦¬ì–´ ì½”ë“œ)\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"âœ… ì´ì „ VRAM ìºì‹œë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤. 0GBì—ì„œ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "# [ìµœì í™”ëœ í•™ìŠµ íŒŒë¼ë¯¸í„°]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/news-classifier-checkpoints\",\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    # (ìœ ì§€) VRAMì„ 75GB+ë¡œ ì±„ìš°ëŠ” ë°°ì¹˜\n",
        "    per_device_train_batch_size=352,\n",
        "\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    logging_steps=100,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "\n",
        "    fp16=True,\n",
        "\n",
        "    # (â˜… â˜… â˜… ìµœì¢… íŠœë‹ â˜… â˜… â˜…)\n",
        "    # ì‹œìŠ¤í…œì´ 16ê°œëŠ” ê³¼ë„í•˜ê³  12ê°œê°€ ìµœì ì´ë¼ê³  ê²½ê³  (UserWarning)\n",
        "    # ë”°ë¼ì„œ 16 -> 12ë¡œ ìˆ˜ì •\n",
        "    dataloader_num_workers=12,\n",
        "\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds_tokenized,\n",
        "    eval_dataset=val_ds_tokenized,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(f\"ğŸš€ í•™ìŠµ ì‹œì‘ (Batch: {training_args.per_device_train_batch_size}, Workers: {training_args.dataloader_num_workers}, fp16: {training_args.fp16})...\")\n",
        "trainer.train()\n",
        "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 9. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ìµœì¢… í‰ê°€\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ† 9. [ë”¥ëŸ¬ë‹ ëª¨ë¸] ìµœì¢… ëª¨ë¸ í‰ê°€\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"... ë”¥ëŸ¬ë‹ ëª¨ë¸ Test Set í‰ê°€ ì¤‘ ...\")\n",
        "test_results = trainer.evaluate(test_ds_tokenized)\n",
        "print(f\"\\nğŸ¯ Test Set ì •í™•ë„: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"ğŸ¯ Test Set F1ì ìˆ˜: {test_results['eval_f1']:.4f}\\n\")\n",
        "\n",
        "print(\"... ë”¥ëŸ¬ë‹ ëª¨ë¸ Test Set ì˜ˆì¸¡(Prediction) ìƒì„± ì¤‘ ...\")\n",
        "dl_predictions = trainer.predict(test_ds_tokenized)\n",
        "y_pred_dl = np.argmax(dl_predictions.predictions, axis=1)\n",
        "y_true_dl = dl_predictions.label_ids\n",
        "\n",
        "# Classification Report ìƒì„±\n",
        "report_dl = classification_report(\n",
        "    y_true_dl,\n",
        "    y_pred_dl,\n",
        "    labels=all_labels_indices,\n",
        "    target_names=label_encoder.classes_,\n",
        "    digits=4,\n",
        "    zero_division=0,\n",
        "    output_dict=True\n",
        ")\n",
        "# í™”ë©´ì—ë„ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "print(\"\\nğŸ“Š [KLUE-RoBERTa] Test Set í‰ê°€ ê²°ê³¼:\")\n",
        "print(classification_report(\n",
        "    y_true_dl,\n",
        "    y_pred_dl,\n",
        "    labels=all_labels_indices,\n",
        "    target_names=label_encoder.classes_,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "print(\"\\nâœ… ë”¥ëŸ¬ë‹(RoBERTa) ëª¨ë¸ í‰ê°€ ê²°ê³¼ê°€ 'report_dl' ë³€ìˆ˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 10. [ğŸŒŸ ì‹ ê·œ] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™” (â˜… â˜… â˜… ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š 10. [ì¢…í•©] ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™”\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# (â˜… ë©”ëª¨ë¦¬ ìµœì í™”) 6ë²ˆì—ì„œ ì‚­ì œëœ ë³€ìˆ˜ë“¤ ì¬ì‚¬ìš©ì„ ìœ„í•´ ì¬ìƒì„±\n",
        "print(\"... ë² ì´ìŠ¤ë¼ì¸(LR) ëª¨ë¸ ì˜ˆì¸¡ê°’ ì¬ìƒì„± (ì‹œê°í™”ìš©) ...\")\n",
        "X_test_text_viz = test_ds_processed['text']\n",
        "y_true_baseline_viz = test_ds_processed['label']\n",
        "X_test_tfidf_viz = tfidf_vectorizer.transform(X_test_text_viz)\n",
        "y_pred_baseline_viz = lr_clf.predict(X_test_tfidf_viz)\n",
        "del X_test_text_viz, X_test_tfidf_viz # ì‚¬ìš© í›„ ì¦‰ì‹œ ì‚­ì œ\n",
        "gc.collect()\n",
        "print(\"âœ… ë² ì´ìŠ¤ë¼ì¸ ì˜ˆì¸¡ê°’ ì¤€ë¹„ ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# 10-1. ì¢…í•© ì„±ëŠ¥ ë¹„êµí‘œ (DataFrame)\n",
        "print(\"\\n### 1. ì¢…í•© ì„±ëŠ¥ ë¹„êµí‘œ (Weighted Avg.)\\n\")\n",
        "metrics_lr = report_lr['weighted avg']\n",
        "metrics_dl = report_dl['weighted avg']\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Baseline (TF-IDF + LR)': metrics_lr,\n",
        "    'KLUE-RoBERTa': metrics_dl\n",
        "}).T[['precision', 'recall', 'f1-score', 'support']]\n",
        "print(comparison_df.to_string(float_format=\"{:.4f}\".format))\n",
        "\n",
        "\n",
        "# 10-2. ì¢…í•© ì„±ëŠ¥ ì‹œê°í™” (Bar Chart)\n",
        "print(\"\\n\\n### 2. ì¢…í•© ì„±ëŠ¥ ì‹œê°í™” (F1 / Precision / Recall)\\n\")\n",
        "df_long = comparison_df.drop(columns='support').reset_index().melt(\n",
        "    'index', var_name='Metric', value_name='Score'\n",
        ")\n",
        "df_long.rename(columns={'index': 'Model'}, inplace=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df_long, x='Metric', y='Score', hue='Model', palette='muted')\n",
        "plt.title('Baseline vs. KLUE-RoBERTa (Weighted Avg. Metrics)', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metric')\n",
        "plt.legend(title='Model', loc='lower right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 10-3. Side-by-Side í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ë¹„êµ\n",
        "print(\"\\n\\n### 3. ëª¨ë¸ë³„ í˜¼ë™ í–‰ë ¬ ë¹„êµ (Normalized)\\n\")\n",
        "\n",
        "# (â˜… â˜… â˜… ë²„ê·¸ ìˆ˜ì • â˜… â˜… â˜…)\n",
        "# 'ticklabels='ë¥¼ 'xticklabels='ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
        "def plot_confusion_matrix_custom(ax, y_true, y_pred, classes, title):\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', square=True,\n",
        "                xticklabels=classes, # â˜… ìˆ˜ì •ë¨\n",
        "                yticklabels=classes,\n",
        "                cbar=False, ax=ax)\n",
        "    ax.set_ylabel('Actual Label', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=14, pad=20)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# 1ë²ˆ í”Œë¡¯: ë² ì´ìŠ¤ë¼ì¸ (LR)\n",
        "plot_confusion_matrix_custom(\n",
        "    axes[0],\n",
        "    y_true_baseline_viz,\n",
        "    y_pred_baseline_viz,\n",
        "    english_labels_ordered,\n",
        "    'Baseline (TF-IDF + LR)'\n",
        ")\n",
        "\n",
        "# 2ë²ˆ í”Œë¡¯: ë”¥ëŸ¬ë‹ (RoBERTa)\n",
        "plot_confusion_matrix_custom(\n",
        "    axes[1],\n",
        "    y_true_dl,\n",
        "    y_pred_dl,\n",
        "    english_labels_ordered,\n",
        "    'KLUE-RoBERTa'\n",
        ")\n",
        "\n",
        "fig.suptitle('Model Comparison: Confusion Matrix (Normalized)', fontsize=18, fontweight='bold', y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "del y_true_baseline_viz, y_pred_baseline_viz, y_true_dl, y_pred_dl\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 11. ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ† 11. ìµœì¢… ëª¨ë¸ ì €ì¥\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "trainer.save_model(str(SAVE_BASE_PATH))\n",
        "tokenizer.save_pretrained(str(SAVE_BASE_PATH))\n",
        "with open(SAVE_BASE_PATH / \"label_encoder.pkl\", 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_BASE_PATH}\")\n",
        "\n",
        "# ë¶ˆí•„ìš”í•œ ì²´í¬í¬ì¸íŠ¸ í´ë” ì‚­ì œ\n",
        "shutil.rmtree(\"/content/news-classifier-checkpoints\", ignore_errors=True)\n",
        "print(\"âœ… ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ\")\n",
        "print(\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
