{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGS1fBXmm48g"
      },
      "source": [
        "## 2. ChatGPT API (1990~2019 Sampling í›„, Zero-Shot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQOZO-NW7aba",
        "outputId": "962c85be-6f8e-4c19-f3de-24526b94922d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/news-classifier/notebooks/03_eval_on_gpt_labels.ipynb\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/news-classifier/notebooks/03_eval_on_gpt_labels.ipynb\n",
        "# ============================================================================\n",
        "# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸ”§ 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\")\n",
        "print(\"=\"*80)\n",
        "!pip install -q --upgrade transformers datasets accelerate scikit-learn polars pandas\n",
        "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\\n\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gc\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Hugging Face\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# TQDMì´ Pandasì™€ ì˜ ì‘ë™í•˜ë„ë¡ ì„¤ì •\n",
        "tqdm.pandas()\n",
        "\n",
        "# ============================================================================\n",
        "# 2. Google Drive ë§ˆìš´íŠ¸ ë° í›ˆë ¨ëœ ëª¨ë¸/3ìš”ì†Œ ë¡œë“œ\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“ 2. Google Drive ë§ˆìš´íŠ¸ ë° í›ˆë ¨ëœ ëª¨ë¸/3ìš”ì†Œ ë¡œë“œ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# í›ˆë ¨ ì‹œ 11ë²ˆ ì„¹ì…˜ì—ì„œ ì €ì¥í–ˆë˜ ê²½ë¡œ\n",
        "SAVE_BASE_PATH = Path(\"/content/drive/MyDrive/best_news_classifier\")\n",
        "if not SAVE_BASE_PATH.exists():\n",
        "    raise FileNotFoundError(f\"âŒ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SAVE_BASE_PATH}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "try:\n",
        "    # 1) ë”¥ëŸ¬ë‹(DL) ëª¨ë¸ ë¡œë“œ\n",
        "    print(\"... 1/5) ë”¥ëŸ¬ë‹(RoBERTa) ëª¨ë¸ ë¡œë“œ ì¤‘ ...\")\n",
        "    dl_model = AutoModelForSequenceClassification.from_pretrained(SAVE_BASE_PATH).to(device)\n",
        "    print(\"âœ… DL ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "    # 2) í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "    print(\"... 2/5) í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(SAVE_BASE_PATH)\n",
        "    print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "    # 3) ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ (DL, LR ê³µí†µ ì‚¬ìš©)\n",
        "    print(\"... 3/5) ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì¤‘ ...\")\n",
        "    with open(SAVE_BASE_PATH / \"label_encoder.pkl\", 'rb') as f:\n",
        "        label_encoder = pickle.load(f)\n",
        "    print(f\"âœ… ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ (í´ë˜ìŠ¤: {label_encoder.classes_})\")\n",
        "\n",
        "    # 4) ë¡œì§€ìŠ¤í‹± íšŒê·€(LR) ëª¨ë¸ ë¡œë“œ\n",
        "    print(\"... 4/5) ë¡œì§€ìŠ¤í‹± íšŒê·€(LR) ëª¨ë¸ ë¡œë“œ ì¤‘ ...\")\n",
        "    with open(SAVE_BASE_PATH / \"baseline_lr.pkl\", 'rb') as f:\n",
        "        lr_clf = pickle.load(f)\n",
        "    print(\"âœ… LR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "    # 5) TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ\n",
        "    print(\"... 5/5) TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ ì¤‘ ...\")\n",
        "    with open(SAVE_BASE_PATH / \"baseline_tfidf_vec.pkl\", 'rb') as f:\n",
        "        tfidf_vectorizer = pickle.load(f)\n",
        "    print(\"âœ… TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nâŒ [ì˜¤ë¥˜] ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"ì´ì „ í•™ìŠµ(11ë²ˆ ì„¹ì…˜)ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€, ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. [â˜…í‰ê°€ ì „ìš©â˜…] ChatGPTë¡œ ë¼ë²¨ë§ëœ ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸš€ 3. ChatGPT ë¼ë²¨ë§ ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# âš ï¸âš ï¸âš ï¸ [ì‚¬ìš©ì ìˆ˜ì • í•„ìš”] âš ï¸âš ï¸âš ï¸\n",
        "# ë¼ë²¨ë§ ì‘ì—…(500, 1000, 3000ê°œ...)ì„ í†µí•´ ìµœì¢… ì €ì¥ëœ parquet íŒŒì¼ ê²½ë¡œ\n",
        "LABELED_DATA_PATH = SAVE_BASE_PATH / \"chatgpt_1000_labels_1990_2019.parquet\"\n",
        "# (ë§Œì•½ 500ê°œ ë²„ì „ì„ ì“°ë ¤ë©´: \"chatgpt_500_labels_1990_2019.parquet\")\n",
        "# (ë§Œì•½ 3000ê°œ ë²„ì „ì„ ì“°ë ¤ë©´: \"chatgpt_3000_labels_1990_2019.parquet\")\n",
        "\n",
        "if not LABELED_DATA_PATH.exists():\n",
        "     raise FileNotFoundError(f\"âŒ ë¼ë²¨ë§ëœ ìƒ˜í”Œ íŒŒì¼({LABELED_DATA_PATH.name})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"âœ… {LABELED_DATA_PATH.name} íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
        "sample_df = pd.read_parquet(LABELED_DATA_PATH)\n",
        "print(f\"âœ… ì´ {len(sample_df)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ.\")\n",
        "\n",
        "# (API í˜¸ì¶œ, í•„í„°ë§, ìƒ˜í”Œ ì¶”ì¶œ, ì €ì¥ ë¡œì§ ëª¨ë‘ ì œê±°ë¨)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. [â˜…í‰ê°€ ì „ìš©â˜…] LR / DL ëª¨ë¸ë¡œ ìƒ˜í”Œ ì¶”ë¡  ì‹¤í–‰\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ”¥ 4. LR / DL ëª¨ë¸ë¡œ {len(sample_df)}ê°œ ìƒ˜í”Œ ì¶”ë¡  ì‹¤í–‰\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- 4-1. ë¡œì§€ìŠ¤í‹± íšŒê·€(LR) ì¶”ë¡  ---\n",
        "print(\"... 1/2) ë¡œì§€ìŠ¤í‹± íšŒê·€(LR) ì¶”ë¡  ì¤‘ ...\")\n",
        "# í›ˆë ¨(5-4)ê³¼ 100% ë™ì¼í•œ ì „ì²˜ë¦¬: [í—¤ë“œë¼ì¸ + \" \" + ë³¸ë¬¸][:1000]\n",
        "X_text_lr = sample_df.apply(\n",
        "    lambda row: f\"{row['headline'] if row['headline'] else ''} {row['content'] if row['content'] else ''}\"[:1000],\n",
        "    axis=1\n",
        ")\n",
        "X_tfidf_lr = tfidf_vectorizer.transform(X_text_lr)\n",
        "lr_pred_indices = lr_clf.predict(X_tfidf_lr)\n",
        "lr_pred_labels = label_encoder.inverse_transform(lr_pred_indices)\n",
        "sample_df['lr_label'] = lr_pred_labels\n",
        "print(\"âœ… LR ì¶”ë¡  ì™„ë£Œ\")\n",
        "\n",
        "# --- 4-2. ë”¥ëŸ¬ë‹(DL) ì¶”ë¡  ---\n",
        "print(\"... 2/2) ë”¥ëŸ¬ë‹(RoBERTa) ì¶”ë¡  ì¤‘ ...\")\n",
        "sample_hf_dataset = Dataset.from_pandas(sample_df)\n",
        "\n",
        "def preprocess_and_tokenize_inference(examples):\n",
        "    h = [t if t else \"\" for t in examples['headline']]\n",
        "    c = [t if t else \"\" for t in examples['content']]\n",
        "    texts = [f\"{hh} {cc}\"[:1000] for hh, cc in zip(h, c)]\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "N_PROC = 4 # ìƒ˜í”Œ ìˆ˜ê°€ ì ìœ¼ë¯€ë¡œ 4ë¡œë„ ì¶©ë¶„\n",
        "sample_ds_tokenized = sample_hf_dataset.map(preprocess_and_tokenize_inference, batched=True, num_proc=N_PROC, remove_columns=sample_hf_dataset.column_names)\n",
        "sample_ds_tokenized.set_format(\"torch\")\n",
        "\n",
        "# L4 ë˜ëŠ” A100ì— ë§ëŠ” ì¶”ë¡  ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/temp_inference_output\",\n",
        "    per_device_eval_batch_size=96,  # L4(24GB) ë˜ëŠ” A100(80GB)ì—ì„œ ë„‰ë„‰í•œ ë°°ì¹˜\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "trainer = Trainer(model=dl_model, args=training_args)\n",
        "\n",
        "dl_predictions_output = trainer.predict(sample_ds_tokenized)\n",
        "dl_pred_indices = np.argmax(dl_predictions_output.predictions, axis=1)\n",
        "dl_pred_labels = label_encoder.inverse_transform(dl_pred_indices)\n",
        "sample_df['dl_label'] = dl_pred_labels\n",
        "print(\"âœ… DL ì¶”ë¡  ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. [â˜…í‰ê°€ ì „ìš©â˜…] ìµœì¢… ì„±ëŠ¥ ë¹„êµ\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ“Š 5. ìµœì¢… ì„±ëŠ¥ ë¹„êµ ({len(sample_df)}ê°œ ìƒ˜í”Œ ê¸°ì¤€)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "valid_samples = sample_df[\n",
        "    (sample_df['chatgpt_label'] != \"ë¶„ë¥˜ì‹¤íŒ¨\") &\n",
        "    (sample_df['chatgpt_label'] != \"ë¶„ë¥˜ì˜¤ë¥˜\")\n",
        "].copy()\n",
        "\n",
        "print(f\"ì´ {len(sample_df)}ê°œ ìƒ˜í”Œ ì¤‘ {len(valid_samples)}ê°œì˜ ìœ íš¨í•œ ìƒ˜í”Œë¡œ ë¹„êµë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\\n\")\n",
        "\n",
        "if len(valid_samples) > 0:\n",
        "    lr_accuracy = accuracy_score(\n",
        "        valid_samples['chatgpt_label'],\n",
        "        valid_samples['lr_label']\n",
        "    )\n",
        "    dl_accuracy = accuracy_score(\n",
        "        valid_samples['chatgpt_label'],\n",
        "        valid_samples['dl_label']\n",
        "    )\n",
        "\n",
        "    print(f\"--- 1990-2019 ë°ì´í„° (GPT-4o-mini ê¸°ì¤€) ---\")\n",
        "    print(f\"ğŸ¤– [ë¡œì§€ìŠ¤í‹± íšŒê·€] ì •í™•ë„: {lr_accuracy:.2%}\")\n",
        "    print(f\"ğŸ”¥ [ë”¥ëŸ¬ë‹ RoBERTa] ì •í™•ë„: {dl_accuracy:.2%}\")\n",
        "else:\n",
        "    print(\"âŒ ë¹„êµí•  ìœ íš¨í•œ ChatGPT ë¶„ë¥˜ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"\\nğŸ‰ ëª¨ë“  í‰ê°€ ì‘ì—… ì™„ë£Œ! ğŸ‰\")\n",
        "print(\"--- [ìµœì¢… ê²°ê³¼ ìƒ˜í”Œ] ---\")\n",
        "print(valid_samples[['headline', 'chatgpt_label', 'lr_label', 'dl_label']].head())\n",
        "\n",
        "# (ìµœì¢… ê²°ê³¼ ì €ì¥ ë¡œì§ì€ ì´ë¯¸ ë¼ë²¨ë§ ìŠ¤í¬ë¦½íŠ¸ì— ìˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ìƒëµí•˜ê±°ë‚˜ ë®ì–´ì“°ê¸°)\n",
        "FINAL_COMPARISON_PATH = SAVE_BASE_PATH / f\"final_{len(sample_df)}_comparison_result.parquet\"\n",
        "valid_samples.to_parquet(FINAL_COMPARISON_PATH, index=False)\n",
        "print(f\"\\nğŸ’¾ ìµœì¢… ë¹„êµ ê²°ê³¼ê°€ {FINAL_COMPARISON_PATH.name} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
